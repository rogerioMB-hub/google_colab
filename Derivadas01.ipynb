{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFcmBm36Y9TyD8rVUCDPO0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rogerioMB-hub/google_colab/blob/main/Derivadas01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9uuAyPUkB0pX"
      },
      "outputs": [],
      "source": [
        "import torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Derivada de função com uma variável"
      ],
      "metadata": {
        "id": "tAOd24X1HyMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)       # uma vez que o tensor ira requerer gradiente...\n",
        "y = (9*x**4) + (2*x**3) + (3*x**2) + (6*x) + 1"
      ],
      "metadata": {
        "id": "it9C3K07H_5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***backward() Method***\n",
        "\n",
        "The graph is differentiated using the chain rule. If the tensor is non-scalar (i.e. its data has more than one element) and requires gradient, the function additionally requires specifying gradient. It should be a tensor of matching type and location, that contains the gradient of the differentiated function w.r.t. self.\n",
        "\n",
        "\n",
        "**The backward() method in Pytorch is used to calculate the gradient during the backward pass in the neural network.** If we do not call this backward() method then gradients are not calculated for the tensors. The gradient of a tensor is calculated for the one having requires_grad is set to True. We can access the gradients using .grad. If we do not call the backward() method or even for the tensors whose requires_grad is set to False, the result is None."
      ],
      "metadata": {
        "id": "hv36My4ZGLhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html\n",
        "\n",
        "https://www.geeksforgeeks.org/python-pytorch-backward-function/"
      ],
      "metadata": {
        "id": "sxu4YP4sGxhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()                                    # computes the gradient of current tensor wrt graph leaves.\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1dluScPCBIx",
        "outputId": "c1349e6d-e153-477b-b143-06c2aa9b421f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(330.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dada a função 9x^4 + 2x^3 + 3x^2 + 6x + 1, sua derivada será 36x^3 + 6x^2 + 6x + 6. Se X assumir valor 2.0, substituindo-o na Y'(x), temos que Y'=36.8 + 6.4 + 6.2 +1 = 330.0"
      ],
      "metadata": {
        "id": "lOgDh0sAC4Hc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Derivadas parciais (mais de uma variável)"
      ],
      "metadata": {
        "id": "R1ttmp1dIIDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(1.0, requires_grad=True)\n",
        "z = torch.tensor(2.0, requires_grad=True)\n",
        "y = x**2 + z**3\n",
        "y.backward()                                    # computes the gradient of y\n",
        "print(x.grad)                                   # prints the partial derivative considering z as a constant\n",
        "print(z.grad)                                   # prints the partial derivative considering x as a constant"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSS3z6oyIWzo",
        "outputId": "e60e6aa5-cba4-42be-8700-b15037095572"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.)\n",
            "tensor(12.)\n"
          ]
        }
      ]
    }
  ]
}